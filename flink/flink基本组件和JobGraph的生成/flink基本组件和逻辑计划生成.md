# Flink åŸºæœ¬ç»„ä»¶å’Œé€»è¾‘è®¡åˆ’ç”Ÿæˆ

## æ¦‚è¦å’ŒèƒŒæ™¯

*flink*æ˜¯ä¸€ä¸ªè¢«èª‰ä¸º *the 4th G* çš„è®¡ç®—æ¡†æ¶ï¼Œä¸åŒçš„æ¡†æ¶ç‰¹æ€§åŠå…¶ä»£è¡¨é¡¹ç›®åˆ—è¡¨å¦‚ä¸‹ï¼š

| ç¬¬ä¸€ä»£              | ç¬¬äºŒä»£                       | ç¬¬ä¸‰ä»£                                      | ç¬¬å››ä»£                                      |
| ---------------- | ------------------------- | ---------------------------------------- | ---------------------------------------- |
| Batch            | **Batch** **Interactive** | **Batch** **Interactive** **Near-Real-Time** **Interative-processing** | **Hybrid** **Interactive** **Real-Time-Streaming** **Native-Iterative-processing** |
|                  | DAG Dataflows             | RDD                                      | Cyclic Dataflows                         |
| Hadoop MapReduce | TEZ                       | Spark                                    | Flink                                    |

æœ¬æ–‡ä¸»è¦ä»‹ç»*flink*çš„æ ¸å¿ƒç»„ä»¶ä»¥åŠç‰©ç†è®¡åˆ’çš„ç”Ÿæˆè¿‡ç¨‹

*å‚è€ƒä»£ç åˆ†æ”¯ flink-1.1.2*

## æ ¸å¿ƒç»„ä»¶ä»‹ç»

*è¿™é‡Œåªä»‹ç» on yarn æ¨¡å¼ä¸‹çš„ç»„ä»¶*

*flink* çš„ on yarn æ¨¡å¼æ”¯æŒä¸¤ç§ä¸åŒçš„ç±»å‹ï¼š

1. å•ä½œä¸šå•é›†ç¾¤
2. å¤šä½œä¸šå•é›†ç¾¤

é¦–å…ˆä»‹ç» *å•ä½œä¸šå•é›†ç¾¤* çš„æ¶æ„ï¼Œå•ä½œä¸šå•é›†ç¾¤ä¸‹ä¸€ä¸ªæ­£å¸¸çš„ *flink* ç¨‹åºä¼šæ‹¥æœ‰ä»¥ä¸‹ç»„ä»¶

---

job Cli: é detatched æ¨¡å¼ä¸‹çš„å®¢æˆ·ç«¯è¿›ç¨‹ï¼Œç”¨ä»¥è·å– yarn Application Master çš„è¿è¡ŒçŠ¶æ€å¹¶å°†æ—¥å¿—è¾“å‡ºæ‰ç»ˆç«¯

JobManager[JM]: è´Ÿè´£ä½œä¸šçš„è¿è¡Œæ—¶è®¡åˆ’ ExecutionGraph çš„ç”Ÿæˆã€ç‰©ç†è®¡åˆ’ç”Ÿæˆå’Œä½œä¸šè°ƒåº¦

TaskManager[TM]: è´Ÿè´£è¢«åˆ†å‘ task çš„æ‰§è¡Œã€å¿ƒè·³/çŠ¶æ€ä¸ŠæŠ¥ã€èµ„æºç®¡ç†

---

æ•´ä½“çš„æ¶æ„å¤§è‡´å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![flink on yarn](flink-on-yarn-arch.png)

ä¸‹é¢å°†ä»¥ä¸€æ¬¡ Job çš„æäº¤è¿‡ç¨‹æè¿° *flink* çš„å„ç»„ä»¶çš„ä½œç”¨åŠååŒ

### ä½œä¸šæäº¤æµç¨‹åˆ†æ

å•ä½œä¸šå•é›†ç¾¤æ¨¡å¼ä¸‹ï¼Œä¸€ä¸ªä½œä¸šä¼šå¯åŠ¨ä¸€ä¸ª JMï¼Œå¹¶ä¾æ®ç”¨æˆ·çš„å‚æ•°ä¼ é€’å¯åŠ¨ç›¸åº”æ•°é‡çš„ TMï¼Œæ¯ä¸ª TM è¿è¡Œåœ¨ yarn çš„ä¸€ä¸ª container ä¸­ï¼Œ

ä¸€ä¸ªé€šå¸¸çš„ flink on yarn æäº¤å‘½ä»¤ï¼š`./bin/flinkÂ runÂ -mÂ yarn-clusterÂ -ynÂ 2 -jÂ flink-demo-1.0.0-with-dependencies.jarÂ â€”ytm 1024 -yst 4 -yjm 1024 â€”yarnname flink_demo_waimai_e` *flink* åœ¨æ”¶åˆ°è¿™æ ·ä¸€æ¡å‘½ä»¤åä¼šé¦–å…ˆé€šè¿‡ Cli è·å– flink çš„é…ç½®ï¼Œå¹¶è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚

#### é…ç½®åŠ è½½

`CliFrontend.java` æ˜¯ flink æäº¤ä½œä¸šçš„å…¥å£

```java
//CliFrontend line144
public CliFrontend() throws Exception {
   this(getConfigurationDirectoryFromEnv());
}
```

è¿™é‡Œä¼šå°è¯•åŠ è½½ conf æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ yaml æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶çš„å‘½åå¹¶æ²¡æœ‰å¼ºåˆ¶é™åˆ¶

#### å‚æ•°è§£æ

è§£æå‘½ä»¤è¡Œå‚æ•°çš„ç¬¬ä¸€æ­¥æ˜¯è·¯ç”±ç”¨æˆ·çš„å‘½ä»¤ï¼Œç„¶åäº¤ç”±`run`æ–¹æ³•å»å¤„ç†

```java
//CliFrontend line993
try {
    return SecurityUtils.runSecured(new SecurityUtils.FlinkSecuredRunner<Integer>() {
	    Override
	    public Integer run() throws Exception {
	        return CliFrontend.this.run(params);
		});
	}
	catch (Exception e) {
		return handleError(e);
	}
```

æ¥ä¸‹æ¥æ˜¯ç¨‹åºå‚æ•°è®¾ç½®è¿‡ç¨‹ï¼Œ*flink* å°† jaråŒ…è·¯å¾„å’Œå‚æ•°é…ç½®å°è£…æˆäº† `PackagedProgram` 

```java
//CliFrontend line223
PackagedProgram program;
try {
   LOG.info("Building program from JAR file");
   program = buildProgram(options);
}
catch (Throwable t) {
   return handleError(t);
}
```

#### flinké›†ç¾¤çš„æ„å»º

##### é›†ç¾¤ç±»å‹çš„è§£æ

è·å–å‚æ•°åä¸‹ä¸€æ­¥å°±æ˜¯é›†ç¾¤çš„æ„å»ºå’Œéƒ¨ç½²ï¼Œflink é€šè¿‡ ä¸¤ä¸ªä¸åŒçš„ `CustomCommandLine ` æ¥å®ç°ä¸åŒé›†ç¾¤æ¨¡å¼çš„è§£æï¼Œåˆ†åˆ«æ˜¯ `FlinkYarnSessionCli`å’Œ `DefaultCLI` ã€åæ§½ä¸€ä¸‹ flink ç±»åçš„å‘½åè§„èŒƒã€‘è§£æå‘½ä»¤è¡Œå‚æ•°

```java
//CliFrontend line125
static {
   /** command line interface of the YARN session, with a special initialization here
    *  to prefix all options with y/yarn. */
   loadCustomCommandLine("org.apache.flink.yarn.cli.FlinkYarnSessionCli", "y", "yarn");
   customCommandLine.add(new DefaultCLI());
}
...
//line882 è¿™é‡Œå°†å†³å®šCliçš„ç±»å‹
CustomCommandLine<?> activeCommandLine = getActiveCustomCommandLine(options.getCommandLine());
```

é‚£ä¹ˆä»€ä¹ˆæ—¶å€™è§£ææˆ Yarn Cluster ä»€ä¹ˆæ—¶å€™è§£ææˆ Standalone å‘¢ï¼Ÿç”±äº`FlinkYarnSessionCli`è¢«ä¼˜å…ˆæ·»åŠ åˆ°`customCommandLine`,æ‰€ä»¥ä¼šå…ˆè§¦å‘ä¸‹é¢è¿™æ®µé€»è¾‘

```java
//FlinkYarnSessionCli line469
@Override
public boolean isActive(CommandLine commandLine, Configuration configuration) {
   String jobManagerOption = commandLine.getOptionValue(ADDRESS_OPTION.getOpt(), null);
   boolean yarnJobManager = ID.equals(jobManagerOption);
   boolean yarnAppId = commandLine.hasOption(APPLICATION_ID.getOpt());
   return yarnJobManager || yarnAppId || loadYarnPropertiesFile(commandLine, configuration) != null;
}
```

ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºå¦‚æœç”¨æˆ·ä¼ å…¥äº† `-m`å‚æ•°æˆ–è€…`application id`æˆ–è€…é…ç½®äº†yarn properties æ–‡ä»¶ï¼Œåˆ™å¯åŠ¨yarn clusteræ¨¡å¼ï¼Œå¦åˆ™æ˜¯Standaloneæ¨¡å¼çš„é›†ç¾¤

##### é›†ç¾¤éƒ¨ç½²

flinké€šè¿‡`YarnClusterDescriptor`æ¥æè¿°yarné›†ç¾¤çš„éƒ¨ç½²é…ç½®ï¼Œå…·ä½“å¯¹åº”çš„é…ç½®æ–‡ä»¶ä¸º`flink-conf.yaml`ï¼Œé€šè¿‡ä¸‹é¢è¿™æ®µé€»è¾‘è§¦å‘é›†ç¾¤éƒ¨ç½²ï¼š

```java
//AbstractYarnClusterDescriptor line372
/**
 * This method will block until the ApplicationMaster/JobManager have been
 * deployed on YARN.
 */
protected YarnClusterClient deployInternal() throws Exception {
```

å¤§è‡´åˆ—ä¸‹è¿‡ç¨‹ï¼š

- check yarn é›†ç¾¤é˜Ÿåˆ—èµ„æºæ˜¯å¦æ»¡è¶³è¯·æ±‚
- è®¾ç½® AM Contextã€å¯åŠ¨å‘½ä»¤ã€submission context
- å¦‚æœå¼€å¯é«˜å¯ç”¨æ¨¡å¼ã€é€šè¿‡åå°„è°ƒç”¨ submission context çš„ä¸¤ä¸ªæ–¹æ³•ä¿®æ”¹å±æ€§ã€‘ keepContainersMethod Â Â  attemptFailuresValidityIntervalMethod ã€å’Œ Hadoop çš„ç‰ˆæœ¬æœ‰å…³ã€‘ç¬¬ä¸€ä¸ªå±æ€§è¡¨ç¤ºåº”ç”¨é‡è¯•æ—¶æ˜¯å¦ä¿ç•™ AM containerï¼Œç¬¬äºŒä¸ªå±æ€§è¡¨ç¤º æŒ‡å®š é—´éš”æ—¶é—´ä¹‹å†…åº”ç”¨å…è®¸å¤±è´¥é‡å¯çš„æ¬¡æ•°
- ä¸Šä¼  ç”¨æˆ· jarã€flink-conf.yamlã€lib ç›®å½•ä¸‹æ‰€æœ‰çš„ jar åŒ…ã€logback log4jé…ç½®æ–‡ä»¶ åˆ° HDFS
- é€šè¿‡ yarn client submit am context
- å°†yarn client åŠç›¸å…³é…ç½®å°è£…æˆ YarnClusterClient è¿”å›

çœŸæ­£åœ¨ AM ä¸­è¿è¡Œçš„ä¸»ç±»æ˜¯ `YarnApplicationMasterRunner`ï¼Œå®ƒçš„ `run`æ–¹æ³•åšäº†å¦‚ä¸‹å·¥ä½œï¼š

-  å¯åŠ¨JobManager ActorSystem
-  å¯åŠ¨ flink ui
-  å¯åŠ¨`YarnFlinkResourceManager`æ¥è´Ÿè´£ä¸yarnçš„ResourceManageräº¤äº’ï¼Œç®¡ç†yarnèµ„æº
-  å¯åŠ¨ actor System supervise è¿›ç¨‹

åˆ°è¿™é‡Œ JobManager å·²ç»å¯åŠ¨èµ·æ¥ï¼Œé‚£ä¹ˆ TaskManageræ˜¯ä»€ä¹ˆæ—¶å€™èµ·åŠ¨çš„å‘¢ï¼Ÿ

åœ¨ `YarnFlinkResourceManager`å¯åŠ¨çš„æ—¶å€™ä¼šé¢„å…ˆæ‰§è¡Œä¸€æ®µé€»è¾‘ã€Akka actorçš„preStartæ–¹æ³•ã€‘ï¼š

```java
@Override
public void preStart() {
    try {
        // we start our leader retrieval service to make sure we get informed
        // about JobManager leader changes
        leaderRetriever.start(new LeaderRetrievalListener() {

		    @Override
		    public void notifyLeaderAddress(String leaderAddress, UUID leaderSessionID) {
		        self().tell(
						new NewLeaderAvailable(leaderAddress, leaderSessionID),
						ActorRef.noSender());
		    }
```

è¿™æ®µé€»è¾‘ä¼šå…ˆå°è¯•è·å– JobManager çš„åœ°å€å¹¶ç»™è‡ªå·±å‘é€ä¸€ä¸ªè·¯ç”±æ¶ˆæ¯`NewLeaderAvailable`ï¼Œç„¶å`YarnFlinkResourceManager`ä¼šæŠŠè‡ªå·±æ³¨å†Œåˆ° `JobManager` ä¸­ï¼Œæ¥ç€`JobManager`ä¼šå‘é€ä¸€ä¸ªå›è°ƒå‘½ä»¤ï¼š

```scala
//JobManager line358
sender ! decorateMessage(new RegisterResourceManagerSuccessful(self, taskManagerResources))
```

æ¥ç€ä¼šè§¦å‘è¿™æ ·ä¸€æ®µé€»è¾‘ï¼š

```java
//FlinkResourceManager line555
private void checkWorkersPool() {
   int numWorkersPending = getNumWorkerRequestsPending();
   int numWorkersPendingRegistration = getNumWorkersPendingRegistration();

   // sanity checks
   Preconditions.checkState(numWorkersPending >= 0,
      "Number of pending workers should never be below 0.");
   Preconditions.checkState(numWorkersPendingRegistration >= 0,
      "Number of pending workers pending registration should never be below 0.");

   // see how many workers we want, and whether we have enough
   int allAvailableAndPending = startedWorkers.size() +
      numWorkersPending + numWorkersPendingRegistration;

   int missing = designatedPoolSize - allAvailableAndPending;

   if (missing > 0) {
      requestNewWorkers(missing);
   }
}
```

å°†æ‰€æœ‰çš„ TS èµ·åŠ¨èµ·æ¥ï¼Œè¿™æ ·ä¸€ä¸ª flink é›†ç¾¤ä¾¿æ„å»ºå‡ºæ¥äº†ã€‚ä¸‹é¢é™„å›¾è§£é‡Šä¸‹è¿™ä¸ªæµç¨‹ï¼š

![flink-cluster-start-flow](flink-cluster-start-flow.png)

1. flink cli è§£ææœ¬åœ°ç¯å¢ƒé…ç½®ï¼Œå¯åŠ¨ `ApplicationMaster`
2. åœ¨ `ApplicationMaster` ä¸­å¯åŠ¨ `JobManager`
3. åœ¨ `ApplicationMaster` ä¸­å¯åŠ¨`YarnFlinkResourceManager`
4. `YarnFlinkResourceManager`ç»™`JobManager`å‘é€æ³¨å†Œä¿¡æ¯
5. `YarnFlinkResourceManager`æ³¨å†ŒæˆåŠŸåï¼Œ`JobManager`ç»™`YarnFlinkResourceManager`å‘é€æ³¨å†ŒæˆåŠŸä¿¡æ¯
6. `YarnFlinkResourceManage`çŸ¥é“è‡ªå·±æ³¨å†ŒæˆåŠŸååƒ`ResourceManager`ç”³è¯·å’Œ`TaskManager`æ•°é‡å¯¹ç­‰çš„ container
7. åœ¨containerä¸­å¯åŠ¨`TaskManager`
8. `TaskManager`å°†è‡ªå·±æ³¨å†Œåˆ°`JobManager`ä¸­

*æ¥ä¸‹æ¥ä¾¿æ˜¯ç¨‹åºçš„æäº¤å’Œè¿è¡Œ*

ç¨‹åºåœ¨`CliFrontend`ä¸­è¢«æäº¤åï¼Œä¼šè§¦å‘è¿™æ ·ä¸€æ®µé€»è¾‘

```java
//ClusterClient 304
	public JobSubmissionResult run(PackagedProgram prog, int parallelism)
			throws ProgramInvocationException
	{
		Thread.currentThread().setContextClassLoader(prog.getUserCodeClassLoader());
		...
		else if (prog.isUsingInteractiveMode()) {
			LOG.info("Starting program in interactive mode");
			ContextEnvironmentFactory factory = new ContextEnvironmentFactory(this, prog.getAllLibraries(),
					prog.getClasspaths(), prog.getUserCodeClassLoader(), parallelism, isDetached(),
					prog.getSavepointPath());
			ContextEnvironment.setAsContext(factory);

			try {
				// invoke main method
				prog.invokeInteractiveModeForExecution();
				...
			}
			finally {
				ContextEnvironment.unsetContext();
			}
		}
		else {
			throw new RuntimeException("PackagedProgram does not have a valid invocation mode.");
		}
	}
```

æ³¨æ„åˆ°æœ‰ä¸€æ®µ`prog.invokeInteractiveModeForExecution()`ï¼Œè¿™æ˜¯å®¢æˆ·ç«¯ç”Ÿæˆåˆæ­¥é€»è¾‘è®¡åˆ’çš„æ ¸å¿ƒé€»è¾‘ï¼Œä¸‹é¢å°†è¯¦ç»†ä»‹ç»

### å®¢æˆ·ç«¯é€»è¾‘è®¡åˆ’

ä¸Šé¢æåˆ°`prog.invokeInteractiveModeForExecution()`è¿™æ®µé€»è¾‘ä¼šè§¦å‘å®¢æˆ·ç«¯é€»è¾‘è®¡åˆ’çš„ç”Ÿæˆï¼Œé‚£ä¹ˆæ˜¯æ€æ ·ä¸€ä¸ªè¿‡ç¨‹å‘¢ï¼Ÿå…¶å®è¿™é‡Œåªæ˜¯è°ƒç”¨äº†ç”¨æˆ·jaråŒ…çš„ä¸»å‡½æ•°ï¼ŒçœŸæ­£çš„è§¦å‘ç”Ÿæˆè¿‡ç¨‹ç”±ç”¨æˆ·ä»£ç çš„æ‰§è¡Œæ¥å®Œæˆã€‚ä¾‹å¦‚ç”¨æˆ·å†™äº†è¿™æ ·ä¸€æ®µ flink ä»£ç ï¼š

```java
object FlinkDemo extends App with Logging{
  override def main(args: Array[String]): Unit ={
    val properties = new Properties
    properties.setProperty("bootstrap.servers", DemoConfig.kafkaBrokerList)

 properties.setProperty("zookeeper.connect","host01:2181,host02:2181,host03:2181/kafka08")
    properties.setProperty("group.id", "flink-demo-waimai-e")

    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.enableCheckpointing(5000L, CheckpointingMode.EXACTLY_ONCE) //checkpoint every 5 seconds.
    val stream = env.addSource(new FlinkKafkaConsumer08[String]("log.waimai_e", new SimpleStringSchema, properties)).setParallelism(2)
    val counts = stream.name("log.waimai_e").map(toPoiIdTuple(_)).filter(_._2 != null)
      .keyBy(0)
      .timeWindow(Time.seconds(5))
      .sum(1)

    counts.addSink(sendToKafka(_))
    env.execute()
  }
```

æ³¨æ„åˆ°è¿™æ ·ä¸€æ®µ`val env = StreamExecutionEnvironment.getExecutionEnvironment`ï¼Œè¿™æ®µä»£ç ä¼šè·å–å®¢æˆ·ç«¯çš„ç¯å¢ƒé…ç½®ï¼Œå®ƒé¦–å…ˆä¼šè½¬åˆ°è¿™æ ·ä¸€æ®µé€»è¾‘ï¼š

```java
//StreamExecutionEnvironment 1256
public static StreamExecutionEnvironment getExecutionEnvironment() {
   if (contextEnvironmentFactory != null) {
      return contextEnvironmentFactory.createExecutionEnvironment();
   }

   // because the streaming project depends on "flink-clients" (and not the other way around)
   // we currently need to intercept the data set environment and create a dependent stream env.
   // this should be fixed once we rework the project dependencies
   
   ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
```

`ExecutionEnvironment.getExecutionEnvironment();`è·å–ç¯å¢ƒçš„é€»è¾‘å¦‚ä¸‹ï¼š

```java
//ExecutionEnvironment line1137
public static ExecutionEnvironment getExecutionEnvironment() {
   return contextEnvironmentFactory == null ? 
         createLocalEnvironment() : contextEnvironmentFactory.createExecutionEnvironment();
}
```

è¿™é‡Œçš„`contextEnvironmentFactory`æ˜¯ä¸€ä¸ªé™æ€æˆå‘˜ï¼Œæ—©åœ¨`ContextEnvironment.setAsContext(factory)`å·²ç»è§¦å‘è¿‡åˆå§‹åŒ–äº†ï¼Œå…¶ä¸­åŒ…å«äº†å¦‚ä¸‹çš„ç¯å¢ƒä¿¡æ¯:

```java
//ContextEnvironmentFactory line51
public ContextEnvironmentFactory(ClusterClient client, List<URL> jarFilesToAttach,
      List<URL> classpathsToAttach, ClassLoader userCodeClassLoader, int defaultParallelism,
      boolean isDetached, String savepointPath)
{
   this.client = client;
   this.jarFilesToAttach = jarFilesToAttach;
   this.classpathsToAttach = classpathsToAttach;
   this.userCodeClassLoader = userCodeClassLoader;
   this.defaultParallelism = defaultParallelism;
   this.isDetached = isDetached;
   this.savepointPath = savepointPath;
}
```

å…¶ä¸­çš„ client å°±æ˜¯ä¸Šé¢ç”Ÿæˆçš„ `YarnClusterClient`ï¼Œå…¶å®ƒçš„æ„æ€è¾ƒæ˜æ˜¾ï¼Œå°±ä¸å¤šåšè§£é‡Šäº†ã€‚

ç”¨æˆ·åœ¨æ‰§è¡Œ`val env = StreamExecutionEnvironment.getExecutionEnvironment`è¿™æ ·ä¸€æ®µé€»è¾‘åä¼šå¾—åˆ°ä¸€ä¸ª`StreamContextEnvironment`ï¼Œå…¶ä¸­å°è£…äº† streaming çš„ä¸€äº›æ‰§è¡Œé…ç½® ã€buffer time outç­‰ã€‘ï¼Œå¦å¤–ä¿å­˜äº†ä¸Šé¢æåˆ°çš„ `ContextEnvironmen`t çš„å¼•ç”¨ã€‚

åˆ°è¿™é‡Œå…³äº streaming éœ€è¦çš„æ‰§è¡Œç¯å¢ƒä¿¡æ¯å·²ç»è®¾ç½®å®Œæˆã€‚

#### åˆæ­¥é€»è¾‘è®¡åˆ’ StreamGraph çš„ç”Ÿæˆ

æ¥ä¸‹æ¥ç”¨æˆ·ä»£ç æ‰§è¡Œåˆ°`val stream = env.addSource(new FlinkKafkaConsumer08`ï¼Œè¿™æ®µé€»è¾‘å®é™…ä¼šç”Ÿæˆä¸€ä¸ª`DataStream`æŠ½è±¡ï¼Œ`DataStream`æ˜¯flinkå…³äºstreamingæŠ½è±¡çš„æœ€æ ¸å¿ƒæŠ½è±¡ï¼Œåç»­æ‰€æœ‰çš„ç®—å­è½¬æ¢éƒ½ä¼šåœ¨`DataStream`ä¸Šæ¥å®Œæˆï¼Œä¸Šé¢çš„`addSource`æ“ä½œä¼šè§¦å‘ä¸‹é¢è¿™æ®µé€»è¾‘:

```java
public <OUT> DataStreamSource<OUT> addSource(SourceFunction<OUT> function, String sourceName, TypeInformation<OUT> typeInfo) {

   if(typeInfo == null) {
      if (function instanceof ResultTypeQueryable) {
         typeInfo = ((ResultTypeQueryable<OUT>) function).getProducedType();
      } else {
         try {
            typeInfo = TypeExtractor.createTypeInfo(
                  SourceFunction.class,
                  function.getClass(), 0, null, null);
         } catch (final InvalidTypesException e) {
            typeInfo = (TypeInformation<OUT>) new MissingTypeInfo(sourceName, e);
         }
      }
   }

   boolean isParallel = function instanceof ParallelSourceFunction;

   clean(function);
   StreamSource<OUT, ?> sourceOperator;
   if (function instanceof StoppableFunction) {
      sourceOperator = new StoppableStreamSource<>(cast2StoppableSourceFunction(function));
   } else {
      sourceOperator = new StreamSource<>(function);
   }

   return new DataStreamSource<>(this, typeInfo, sourceOperator, isParallel, sourceName);
}
```

ç®€è¦æ€»ç»“ä¸‹ä¸Šé¢çš„é€»è¾‘ï¼š

- è·å–æ•°æ®æº source çš„ output ä¿¡æ¯ TypeInformation
- ç”Ÿæˆ StreamSource sourceOperator
- ç”Ÿæˆ DataStreamSourceã€å°è£…äº† sourceOperatorã€‘ï¼Œå¹¶è¿”å›
- å°† StreamTransformation æ·»åŠ åˆ°ç®—å­åˆ—è¡¨ transformations ä¸­ã€åªæœ‰ è½¬æ¢ transform æ“ä½œæ‰ä¼šæ·»åŠ ç®—å­ï¼Œå…¶å®ƒéƒ½åªæ˜¯æš‚æ—¶åšäº† transformation çš„å åŠ å°è£…ã€‘
- åç»­ä¼šåœ¨ DataStream ä¸Šåšæ“ä½œ

è¯¥è¾“å‡º`DataStreamSource`ç»§æ‰¿è‡ª`SingleOutputStreamOperator`å…·ä½“çš„ç»§æ‰¿å…³ç³»å¦‚ä¸‹ï¼š

![flink-datastream-extend](flink-datastream-extend.png)

è€Œç”Ÿæˆçš„ StreamSource operator èµ°çš„æ˜¯å¦ä¸€å¥—ç»§æ‰¿æ¥å£ï¼š

![stream-operator-extend.png](stream-operator-extend.png)

DataStreamSource æ˜¯ä¸€ä¸ª DataStream **æ•°æ®æµ**æŠ½è±¡ï¼ŒStreamSource æ˜¯ä¸€ä¸ª StreamOperator **ç®—å­**æŠ½è±¡ï¼Œåœ¨ flink ä¸­ä¸€ä¸ª DataStream å°è£…äº†ä¸€æ¬¡æ•°æ®æµè½¬æ¢ï¼Œä¸€ä¸ª StreamOperator å°è£…äº†ä¸€ä¸ªå‡½æ•°æ¥å£ï¼Œæ¯”å¦‚ mapã€reduceã€keyByç­‰ã€‚*å…³äºç®—å­çš„ä»‹ç»ä¼šå¦èµ·ä¸€èŠ‚ï¼šflinkç®—å­çš„å£°æ˜å‘¨æœŸ*

å¯ä»¥çœ‹åˆ°åœ¨ DataStream ä¸Šå¯ä»¥è¿›è¡Œä¸€ç³»åˆ—çš„æ“ä½œ(map filter ç­‰)ï¼Œæ¥çœ‹ä¸€ä¸ªå¸¸è§„æ“ä½œæ¯”å¦‚ map ä¼šå‘ç”Ÿä»€ä¹ˆï¼š

```java
//DataStream line503
public <R> SingleOutputStreamOperator<R> map(MapFunction<T, R> mapper) {

   TypeInformation<R> outType = TypeExtractor.getMapReturnTypes(clean(mapper), getType(),
         Utils.getCallLocationName(), true);

   return transform("Map", outType, new StreamMap<>(clean(mapper)));
}
```

ä¸€ä¸ªmapæ“ä½œä¼šè§¦å‘ä¸€æ¬¡ transformï¼Œé‚£ä¹ˆtransformåšäº†ä»€ä¹ˆå·¥ä½œå‘¢ï¼Ÿ

```java
//DataStream line1020
@PublicEvolving
public <R> SingleOutputStreamOperator<R> transform(String operatorName, TypeInformation<R> outTypeInfo, OneInputStreamOperator<T, R> operator) {

   // read the output type of the input Transform to coax out errors about MissingTypeInfo
   transformation.getOutputType();

   OneInputTransformation<T, R> resultTransform = new OneInputTransformation<>(
         this.transformation,
         operatorName,
         operator,
         outTypeInfo,
         environment.getParallelism());

   @SuppressWarnings({ "unchecked", "rawtypes" })
   SingleOutputStreamOperator<R> returnStream = new SingleOutputStreamOperator(environment, resultTransform);

   getExecutionEnvironment().addOperator(resultTransform);

   return returnStream;
}
```

è¿™ä¸€æ­¥ç”Ÿæˆäº†ä¸€ä¸ª `StreamTransformation`å¹¶ä»¥æ­¤ä½œä¸ºæˆå‘˜å˜é‡å°è£…æˆå¦ä¸€ä¸ª DataStream è¿”å›ï¼Œ`StreamTransformation`æ˜¯ flinkå…³äºæ•°æ®æµè½¬æ¢çš„æ ¸å¿ƒæŠ½è±¡ï¼Œåªæœ‰éœ€è¦ transform çš„æµæ‰ä¼šç”Ÿæˆæ–°çš„DataStream ç®—å­ï¼Œåé¢ä¼šè¯¦ç»†è§£é‡Šï¼Œæ³¨æ„ä¸Šé¢æœ‰è¿™ä¸€è¡Œ`getExecutionEnvironment().addOperator(resultTransform)`flinkä¼šå°†transformationç»´æŠ¤èµ·æ¥ï¼š

```java
//StreamExecutionEnvironment line 1237
@Internal
public void addOperator(StreamTransformation<?> transformation) {
   Preconditions.checkNotNull(transformation, "transformation must not be null.");
   this.transformations.add(transformation);
}
```

æ‰€ä»¥ï¼Œç”¨æˆ·çš„ä¸€è¿ä¸²æ“ä½œ map joinç­‰å®é™…ä¸Šåœ¨ DataStream ä¸Šåšäº†è½¬æ¢ï¼Œå¹¶ä¸”flinkå°†è¿™äº› `StreamTransformation` ç»´æŠ¤èµ·æ¥ï¼Œä¸€ç›´åˆ°æœ€åï¼Œç”¨æˆ·æ‰§è¡Œ `env.execute()`è¿™æ ·ä¸€æ®µé€»è¾‘ï¼ŒStreamGraph çš„æ„å»ºæ‰ç®—çœŸæ­£å¼€å§‹...

ç”¨æˆ·åœ¨æ‰§è¡Œ` env.execute()`ä¼šè§¦å‘è¿™æ ·ä¸€æ®µé€»è¾‘ï¼š

```java
//StreamContextEnvironment line51   
public JobExecutionResult execute(String jobName) throws Exception {
      Preconditions.checkNotNull("Streaming Job name should not be null.");

      StreamGraph streamGraph = this.getStreamGraph();
      streamGraph.setJobName(jobName);

      transformations.clear();

      // execute the programs
      if (ctx instanceof DetachedEnvironment) {
         LOG.warn("Job was executed in detached mode, the results will be available on completion.");
         ((DetachedEnvironment) ctx).setDetachedPlan(streamGraph);
         return DetachedEnvironment.DetachedJobExecutionResult.INSTANCE;
      } else {
         return ctx.getClient().runBlocking(streamGraph, ctx.getJars(), ctx.getClasspaths(), ctx.getUserCodeClassLoader(), ctx.getSavepointPath());
      }
   }
}
```

è¿™æ®µä»£ç åšäº†ä¸¤ä»¶äº‹æƒ…ï¼š

- é¦–å…ˆä½¿ç”¨ `StreamGraphGenerator` äº§ç”Ÿ StreamGraph
- ä½¿ç”¨ Client è¿è¡Œ stream graph

é‚£ä¹ˆ` StreamGraphGenerator` åšäº†å“ªäº›æ“ä½œå‘¢ï¼Ÿ

` StreamGraphGenerator`ä¼šä¾æ®æ·»åŠ ç®—å­æ—¶ä¿å­˜çš„ transformations ä¿¡æ¯ç”Ÿæˆ job graph ä¸­çš„èŠ‚ç‚¹ï¼Œå¹¶åˆ›å»ºèŠ‚ç‚¹è¿æ¥ï¼Œåˆ†æµæ“ä½œ å¦‚ union,select,split ä¸ä¼šæ·»åŠ è¾¹ï¼Œåªä¼šåˆ›å»ºè™šæ‹ŸèŠ‚ç‚¹æˆ–åœ¨ä¸Šæœ‰èŠ‚ç‚¹æ·»åŠ  selector

è¿™é‡Œä¼šå°† StreamTransformation è½¬æ¢ä¸º StreamNodeï¼ŒStreamNode ä¿å­˜äº†ç®—å­çš„ä¿¡æ¯ã€ä¼šå¦å¤–ä»‹ç»ã€‘ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º

<img src="./transformation-to-node.png" width="535" height="300" alt="transformation-to-node.png" align=center />

åˆ°è¿™é‡Œç”± `StreamNode` æ„æˆçš„ DAG å›¾ `StreamGraph`å°±ç”Ÿæˆäº†

ä¸è¿‡ åœ¨æäº¤ç»™ client çš„æ—¶å€™ï¼Œflink ä¼šåšè¿›ä¸€æ­¥çš„ä¼˜åŒ–:

 `StreamGraph` å°†è¿›ä¸€æ­¥è½¬æ¢ä¸º `JobGraph`ï¼Œè¿™ä¸€æ­¥å·¥ä½œç”± `StreamingJobGraphGenerator` æ¥å®Œæˆï¼Œä¸ºä»€ä¹ˆè¦åšè¿™ä¸€æ­¥è½¬æ¢å‘¢ï¼Ÿä¸»è¦å› ä¸ºæœ‰å¯ä»¥ chain çš„ç®—å­ï¼Œè¿™é‡Œè¿›ä¸€æ­¥å°† StreamNode è½¬æ¢ä¸º JobVertexï¼Œä¸»è¦å·¥ä½œæ˜¯å°†å¯ä»¥ chain çš„ç®—å­åˆå¹¶ã€è¿™ä¸€æ­¥ä¼˜åŒ–æ˜¯é»˜è®¤æ‰“å¼€çš„ã€‘ï¼Œå¹¶è®¾ç½®èµ„æºï¼Œé‡è¯•ç­–ç•¥ç­‰ï¼Œæœ€ç»ˆç”Ÿæˆå¯ä»¥æäº¤ç»™ JobManager çš„ JobGraph

#### ä¼˜åŒ–çš„é€»è¾‘è®¡åˆ’ JobGraph çš„ç”Ÿæˆ

```java
//StreamingJobGraphGenerator line181
private List<StreamEdge> createChain(
      Integer startNodeId,
      Integer currentNodeId,
      Map<Integer, byte[]> hashes) {

   if (!builtVertices.contains(startNodeId)) {

      List<StreamEdge> transitiveOutEdges = new ArrayList<StreamEdge>();

      List<StreamEdge> chainableOutputs = new ArrayList<StreamEdge>();
      List<StreamEdge> nonChainableOutputs = new ArrayList<StreamEdge>();

      for (StreamEdge outEdge : streamGraph.getStreamNode(currentNodeId).getOutEdges()) {
         if (isChainable(outEdge)) {
            chainableOutputs.add(outEdge);
         } else {
            nonChainableOutputs.add(outEdge);
         }
      }

      for (StreamEdge chainable : chainableOutputs) {
         transitiveOutEdges.addAll(createChain(startNodeId, chainable.getTargetId(), hashes));
      }
     ...
```

ä¸Šé¢çš„æ–¹æ³•æ˜¯ç®—å­ chain çš„æ ¸å¿ƒæ“ä½œï¼Œç®€è¦æ¦‚æ‹¬ä¸‹ï¼š

- å¦‚æœä»æ­¤ start node å¼€å§‹æœªç”Ÿæˆè¿‡ JobVertexï¼Œåˆ™æ‰§è¡Œ chainé€»è¾‘ï¼Œç”±äºæ˜¯é€’å½’æ“ä½œï¼Œä¼šå…ˆæ·±åº¦ä¼˜å…ˆéå†ï¼Œå°†æºèŠ‚ç‚¹å¼€å§‹åˆ°ç¬¬ä¸€ä¸ªä¸å¯ chain çš„ StreamNode ä¹‹é—´çš„ç®—å­åš chain æ“ä½œã€å…ˆç®—å¶å­èŠ‚ç‚¹çš„ chainï¼Œä¾æ¬¡å¾€æ ¹èŠ‚ç‚¹è®¡ç®—ã€‘
- line 207 é‡åˆ°ä¸å¯ chain çš„è¾¹ï¼Œå¼€å§‹æ·±åº¦éå†ç”Ÿæˆ JobVertex
- line 216 å°† StreamNode çš„è¾“å…¥è¾“å‡ºé…ç½®ï¼ŒåŒ…æ‹¬åºåˆ—åŒ–é…ç½®ç­‰è®¾ç½®åˆ°ä¸Šé¢çš„ StreamingConfig ä¸­ï¼Œå¹¶åœ¨ vertexConfigs ä¸­ä¿å­˜èµ·æ¥ï¼Œå¦‚æœæ˜¯ æ–°ç”Ÿæˆçš„ JobVertexï¼Œèµ·å¯¹åº”çš„ StreamingConfig ä¼šä»¥ start node id ä¸º key è¿›è¡Œä¿å­˜
- transitiveOutEdges ä¿å­˜çš„è¯¥èŠ‚ç‚¹ä¸‹æ¸¸æ‰€æœ‰çš„ non chain_able Â edgesï¼Œæœ€ç»ˆçš„æ–¹æ³•ä¼šè¿”å›æ­¤æ•°æ®ç»“æ„
- è¿æ¥ start node å’Œæ‰€æœ‰çš„Â transitiveOutEdges ã€åœ¨è¾“å…¥ JobVertex åˆ›å»ºÂ IntermediateDataSetï¼Œpartitionç±»å‹ä¸º pipelineï¼Œç”Ÿæˆ JobEdgeã€‘
- å¦‚æœæ˜¯æ–°ç”ŸæˆJobVertexï¼Œç»§ç»­è®¾ç½®configï¼ŒåŒ…æ‹¬ chain startï¼Œæ‰€æœ‰ç‰©ç†è¾“å‡ºï¼ŒåŠç›´æ¥é€»è¾‘è¾“å‡ºã€chained configç­‰
- å¦‚æœä¸æ˜¯æ–°ç”Ÿæˆ JobVertexï¼Œç›´æ¥chain configs

è¿™é‡Œæ€»ç»“ä¸‹JobGraphçš„æ„å»ºè¿‡ç¨‹ï¼Œè§ä¸‹å›¾:

![flink-job-graph-create](flink-job-graph-create.png)

å¤§è‡´è¿‡ç¨‹æ€»ç»“å¦‚ä¸‹ï¼š

- ç”±`DataStream`ä¸Šçš„æ“ä½œç”Ÿæˆ`StreamTransformation`åˆ—è¡¨
- ä»`StreamTransformation`çš„ç”Ÿæˆå…³ç³»åˆ›å»º`StreamNode`å’Œ`StreamEdge`
- åšç®—å­chainï¼Œåˆå¹¶æˆ `JobVertex`ï¼Œå¹¶ç”Ÿæˆ `JobEdge`

ä¸€ä¸ª JobVertex ä»£è¡¨ä¸€ä¸ªé€»è¾‘è®¡åˆ’çš„èŠ‚ç‚¹ï¼Œå°±æ˜¯ DAG å›¾ä¸Šçš„é¡¶ç‚¹ï¼Œæœ‰ç‚¹ç±»ä¼¼äº Storm çš„ bolt æˆ– spoutï¼Œç”Ÿæˆä¸€ä¸ª JobVertex çš„é€»è¾‘å¦‚ä¸‹ï¼š

```java
//StreamingJobGenerator line258
private StreamConfig createJobVertex(
      Integer streamNodeId,
      Map<Integer, byte[]> hashes) {

   JobVertex jobVertex;
   ...
   JobVertexID jobVertexId = new JobVertexID(hash);

   if (streamNode.getInputFormat() != null) {
      jobVertex = new InputFormatVertex(
            chainedNames.get(streamNodeId),
            jobVertexId);
      TaskConfig taskConfig = new TaskConfig(jobVertex.getConfiguration());
      taskConfig.setStubWrapper(new UserCodeObjectWrapper<Object>(streamNode.getInputFormat()));
   } else {
      jobVertex = new JobVertex(
            chainedNames.get(streamNodeId),
            jobVertexId);
   }

   jobVertex.setInvokableClass(streamNode.getJobVertexClass());

   ...

   return new StreamConfig(jobVertex.getConfiguration());
}
```

*è¿™é‡Œæœ‰ä¸¤æ®µé€»è¾‘å€¼å¾—æ³¨æ„ï¼Œç¬¬ä¸€æ˜¯æ•°æ®æºèŠ‚ç‚¹çš„åˆ¤æ–­ï¼Œç¬¬äºŒæ˜¯è¿è¡Œæ—¶æ‰§è¡Œç±» InvokableClass çš„è®¾ç½®*

`streamNode.getInputFormat()`æ˜¯åˆ¤æ–­æ˜¯å¦æ˜¯æ•°æ®æºèŠ‚ç‚¹çš„é€»è¾‘ï¼Œå¦‚æœæ˜¯æ•°æ®æºèŠ‚ç‚¹ï¼Œè¿™é‡Œä¼šå°†ç”¨æˆ·ä»£ç ã€è¿™é‡Œä¸º InputFormat.class çš„å­ç±»ã€‘è®¾ç½®è¿› JobVertex çš„é…ç½®ä¸­ï¼Œå¹¶åœ¨ JobManager æ‰§è¡Œæäº¤ä½œä¸šå‘½ä»¤çš„æ—¶å€™åšåˆå§‹åŒ–ï¼Œä¼šåœ¨ Flink ç‰©ç†è®¡åˆ’ç”Ÿæˆä¸€èŠ‚ä»‹ç»ã€‚

`jobVertex.setInvokableClass`æ˜¯è®¾ç½®è¿è¡Œæ—¶çš„æ‰§è¡Œç±»ï¼Œé€šè¿‡è¿™ä¸ªç±»å†è°ƒç”¨ç”¨æˆ·å®šä¹‰çš„ operatorï¼Œæ˜¯ flink task ä¸­çœŸæ­£è¢«æ‰§è¡Œçš„ç±»ï¼Œå…·ä½“ä¼šåœ¨ flink-task-runtime ä¸€èŠ‚ä¸­è¯¦ç»†ä»‹ç»ã€‚

è‡³æ­¤ JobGraph ç”Ÿæˆï¼Œå¹¶æ‰”ç»™ JobManager æ‰§è¡ŒğŸ˜